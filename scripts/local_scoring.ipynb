{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "header",
      "metadata": {},
      "source": [
        "# Local Churn Scoring Workflow\n",
        "\n",
        "**Complete end-to-end workflow for local scoring and exploration.**\n",
        "\n",
        "This notebook provides all local processing utilities:\n",
        "- **Optional:** Split large CSV files into chunks\n",
        "- **Load** CSV files (single file or directory)\n",
        "- **Score** customers with churn predictions (uses `function_app/scorer.py`)\n",
        "- **Shape** data to match SQL view structure\n",
        "- **Explore** results interactively\n",
        "- **Export** outputs for Power BI exploration\n",
        "\n",
        "**Inputs:** CSV files from `data/` folder (or single file)\n",
        "\n",
        "**Outputs:**\n",
        "- `outputs/churn_scores_combined.csv` - All scored records\n",
        "- `outputs/churn_scores_sql_view.csv` - Shaped to match SQL view\n",
        "- `outputs/README.md` - Project documentation\n",
        "- `outputs/model_conda.yml` - Model environment configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "env",
      "metadata": {},
      "source": [
        "## 0) Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "env-check",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import platform\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"Python:\", sys.version)\n",
        "print(\"Platform:\", platform.platform())\n",
        "\n",
        "# Set project root (notebook is in scripts/)\n",
        "PROJECT_ROOT = Path().resolve().parent\n",
        "print(f\"\\nProject root: {PROJECT_ROOT}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d09ef3d",
      "metadata": {},
      "source": [
        "### Optional: Split Large CSV Files\n",
        "\n",
        "If you have a very large CSV file that needs to be split into smaller chunks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f404136",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Split large CSV file into smaller chunks\n",
        "# Uncomment and adjust if you need to split a large CSV file first\n",
        "\n",
        "# LARGE_CSV_FILE = PROJECT_ROOT / \"path\" / \"to\" / \"large_file.csv\"\n",
        "\n",
        "def split_csv(input_file, rows_per_file=50000, output_dir=\"data\"):\n",
        "    \"\"\"Split a CSV file into smaller chunks.\"\"\"\n",
        "    import os\n",
        "    from typing import cast\n",
        "    import pandas as pd  # Import pandas inside function since it's defined before imports cell\n",
        "    from pandas import DataFrame\n",
        "    \n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    print(f\"Reading {input_file}...\")\n",
        "    # pd.read_csv with file path and low_memory=False always returns DataFrame\n",
        "    # Use cast to inform type checker of the expected type\n",
        "    df = cast(DataFrame, pd.read_csv(input_file, low_memory=False))\n",
        "    total_rows = len(df)\n",
        "    print(f\"Total rows: {total_rows:,}\")\n",
        "    \n",
        "    num_chunks = (total_rows // rows_per_file) + (1 if total_rows % rows_per_file else 0)\n",
        "    print(f\"Splitting into {num_chunks} files of up to {rows_per_file:,} rows each...\")\n",
        "    \n",
        "    base_name = Path(input_file).stem\n",
        "    \n",
        "    for i in range(num_chunks):\n",
        "        start_idx = i * rows_per_file\n",
        "        end_idx = min((i + 1) * rows_per_file, total_rows)\n",
        "        \n",
        "        chunk = df.iloc[start_idx:end_idx]\n",
        "        output_file = Path(output_dir) / f\"{base_name}_part{i+1:03d}.csv\"\n",
        "        \n",
        "        chunk.to_csv(output_file, index=False)\n",
        "        print(f\"  ✓ Wrote {output_file.name} ({len(chunk):,} rows)\")\n",
        "    \n",
        "    print(f\"\\n✓ Done! {num_chunks} files created in {output_dir}/\")\n",
        "    return num_chunks\n",
        "\n",
        "# Uncomment to run (adjust path as needed):\n",
        "# split_csv(LARGE_CSV_FILE, SPLIT_ROWS_PER_FILE, SPLIT_OUTPUT_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "imports",
      "metadata": {},
      "source": [
        "## 1) Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "imports-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import shutil\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "# Add function_app to path for imports\n",
        "sys.path.insert(0, str(PROJECT_ROOT / \"function_app\"))\n",
        "from scorer import score_customers, load_model  # type: ignore[import-untyped]\n",
        "\n",
        "print(\"✓ Imports successful\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "config",
      "metadata": {},
      "source": [
        "## 2) Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "config-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Input: single file, directory, or default to data/\n",
        "INPUT_PATH = PROJECT_ROOT / \"data\"  # Change to specific file if needed: PROJECT_ROOT / \"data\" / \"validate.csv\"\n",
        "\n",
        "# Output paths\n",
        "OUTPUT_DIR = PROJECT_ROOT / \"outputs\"\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "COMBINED_OUTPUT = OUTPUT_DIR / \"churn_scores_combined.csv\"\n",
        "SQL_VIEW_OUTPUT = OUTPUT_DIR / \"churn_scores_sql_view.csv\"\n",
        "\n",
        "print(f\"Input path: {INPUT_PATH}\")\n",
        "print(f\"Output directory: {OUTPUT_DIR}\")\n",
        "print(f\"Combined output: {COMBINED_OUTPUT}\")\n",
        "print(f\"SQL view output: {SQL_VIEW_OUTPUT}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "load-data",
      "metadata": {},
      "source": [
        "## 3) Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "find-files",
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_csv_files(input_path: Path) -> list[Path]:\n",
        "    \"\"\"Find CSV files from input path (file or directory).\"\"\"\n",
        "    if not input_path.exists():\n",
        "        raise FileNotFoundError(f\"Path not found: {input_path}\")\n",
        "    \n",
        "    if input_path.is_file():\n",
        "        if input_path.suffix.lower() != '.csv':\n",
        "            raise ValueError(f\"Input file must be a CSV file: {input_path}\")\n",
        "        return [input_path]\n",
        "    \n",
        "    # It's a directory - find all CSV files\n",
        "    csv_files: list[Path] = sorted(input_path.glob(\"*.csv\"))\n",
        "    \n",
        "    if not csv_files:\n",
        "        raise FileNotFoundError(f\"No CSV files found in {input_path}\")\n",
        "    \n",
        "    return csv_files\n",
        "\n",
        "csv_files: list[Path] = find_csv_files(INPUT_PATH)\n",
        "print(f\"Found {len(csv_files)} CSV file(s) to process:\")\n",
        "for f in csv_files:\n",
        "    print(f\"  - {f.name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "load-dataframes",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load all CSV files\n",
        "all_dataframes = []\n",
        "\n",
        "for csv_file in csv_files:\n",
        "    print(f\"Loading {csv_file.name}...\")\n",
        "    df = pd.read_csv(csv_file, low_memory=False)\n",
        "    df['SourceFile'] = csv_file.name  # Track source file\n",
        "    all_dataframes.append(df)\n",
        "    print(f\"  ✓ Loaded {len(df):,} rows\")\n",
        "\n",
        "print(f\"\\nTotal files loaded: {len(all_dataframes)}\")\n",
        "print(f\"Total rows: {sum(len(df) for df in all_dataframes):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5590ee0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# NOTE: Report generation happens after section 6 (after shaped_df is created)\n",
        "# The generate_model_report function is defined in section 8\n",
        "# It will be called after shaped_df is created in the \"Apply shaping\" cell\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "score",
      "metadata": {},
      "source": [
        "## 4) Score Customers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd16c761",
      "metadata": {},
      "source": [
        "## 8) Generate Model Report\n",
        "\n",
        "Create a comprehensive business-ready report analyzing the model performance and scored data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80a3616e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_model_report(\n",
        "    combined_df: pd.DataFrame,\n",
        "    shaped_df: pd.DataFrame,\n",
        "    model_obj,\n",
        "    output_path: Path\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Generate a comprehensive business-ready model report in markdown format.\n",
        "    \n",
        "    Args:\n",
        "        combined_df: All scored records (all snapshots)\n",
        "        shaped_df: Shaped data (latest snapshot per customer, SQL view format)\n",
        "        model: Loaded XGBoost model\n",
        "        output_path: Path to save the markdown report\n",
        "    \n",
        "    Returns:\n",
        "        The generated report text as a string\n",
        "    \"\"\"\n",
        "    import pandas as pd  # Ensure pd is available in function scope\n",
        "    from datetime import datetime\n",
        "    \n",
        "    report_lines = []\n",
        "    \n",
        "    # Header\n",
        "    report_lines.append(\"# Customer Churn Prediction Model - Business Report\")\n",
        "    report_lines.append(\"\")\n",
        "    report_lines.append(f\"**Report Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    report_lines.append(\"\")\n",
        "    report_lines.append(\"---\")\n",
        "    report_lines.append(\"\")\n",
        "    \n",
        "    # Executive Summary\n",
        "    report_lines.append(\"## Executive Summary\")\n",
        "    report_lines.append(\"\")\n",
        "    report_lines.append(\"This report provides a comprehensive analysis of the customer churn prediction model, \")\n",
        "    report_lines.append(\"including model performance metrics, risk distribution analysis, and business insights \")\n",
        "    report_lines.append(\"derived from the scored customer data.\")\n",
        "    report_lines.append(\"\")\n",
        "    \n",
        "    total_customers = shaped_df['CustomerId'].nunique() if 'CustomerId' in shaped_df.columns else len(shaped_df)\n",
        "    total_records = len(combined_df)\n",
        "    \n",
        "    report_lines.append(f\"- **Total Customers Analyzed:** {total_customers:,}\")\n",
        "    report_lines.append(f\"- **Total Records Scored:** {total_records:,}\")\n",
        "    \n",
        "    if 'SnapshotDate' in combined_df.columns:\n",
        "        date_range = f\"{combined_df['SnapshotDate'].min()} to {combined_df['SnapshotDate'].max()}\"\n",
        "        report_lines.append(f\"- **Date Range:** {date_range}\")\n",
        "    \n",
        "    report_lines.append(\"\")\n",
        "    report_lines.append(\"---\")\n",
        "    report_lines.append(\"\")\n",
        "    \n",
        "    # Model Information\n",
        "    report_lines.append(\"## Model Information\")\n",
        "    report_lines.append(\"\")\n",
        "    report_lines.append(\"### Model Architecture\")\n",
        "    report_lines.append(\"\")\n",
        "    report_lines.append(\"- **Algorithm:** XGBoost (Gradient Boosting)\")\n",
        "    report_lines.append(\"- **Type:** Binary Classification (Churn Risk Prediction)\")\n",
        "    report_lines.append(\"- **Prediction Window:** 90-day churn risk\")\n",
        "    report_lines.append(\"\")\n",
        "    \n",
        "    if model_obj is not None:\n",
        "        try:\n",
        "            # Get model parameters\n",
        "            params = model_obj.get_params()\n",
        "            report_lines.append(\"### Model Parameters\")\n",
        "            report_lines.append(\"\")\n",
        "            key_params = ['n_estimators', 'max_depth', 'learning_rate', 'subsample', 'colsample_bytree']\n",
        "            for param in key_params:\n",
        "                if param in params:\n",
        "                    report_lines.append(f\"- **{param.replace('_', ' ').title()}:** {params[param]}\")\n",
        "            report_lines.append(\"\")\n",
        "        except Exception:\n",
        "            pass\n",
        "    \n",
        "    # Get feature count\n",
        "    feature_count = len([c for c in combined_df.columns \n",
        "                        if c not in ['CustomerId', 'AccountName', 'Segment', 'CostCenter', \n",
        "                                    'SnapshotDate', 'ChurnRiskPct', 'RiskBand', \n",
        "                                    'Reason_1', 'Reason_2', 'Reason_3', 'SourceFile', \n",
        "                                    'ScoredAt', 'FirstPurchaseDate', 'LastPurchaseDate', 'Status']])\n",
        "    report_lines.append(f\"- **Number of Features:** {feature_count}\")\n",
        "    report_lines.append(\"\")\n",
        "    report_lines.append(\"---\")\n",
        "    report_lines.append(\"\")\n",
        "    \n",
        "    # Data Overview\n",
        "    report_lines.append(\"## Data Overview\")\n",
        "    report_lines.append(\"\")\n",
        "    report_lines.append(\"### Dataset Statistics\")\n",
        "    report_lines.append(\"\")\n",
        "    report_lines.append(f\"- **Total Records:** {total_records:,}\")\n",
        "    report_lines.append(f\"- **Unique Customers:** {total_customers:,}\")\n",
        "    \n",
        "    if 'SourceFile' in combined_df.columns:\n",
        "        num_files = combined_df['SourceFile'].nunique()\n",
        "        report_lines.append(f\"- **Source Files Processed:** {num_files}\")\n",
        "        if num_files > 1:\n",
        "            report_lines.append(\"\")\n",
        "            report_lines.append(\"**Files Processed:**\")\n",
        "            file_counts = combined_df['SourceFile'].value_counts()\n",
        "            for filename, count in file_counts.items():\n",
        "                pct = (count / total_records) * 100\n",
        "                report_lines.append(f\"- {filename}: {count:,} records ({pct:.1f}%)\")\n",
        "    \n",
        "    report_lines.append(\"\")\n",
        "    \n",
        "    # Date range analysis\n",
        "    if 'SnapshotDate' in combined_df.columns:\n",
        "        report_lines.append(\"### Temporal Coverage\")\n",
        "        report_lines.append(\"\")\n",
        "        earliest = combined_df['SnapshotDate'].min()\n",
        "        latest = combined_df['SnapshotDate'].max()\n",
        "        report_lines.append(f\"- **Earliest Snapshot:** {earliest}\")\n",
        "        report_lines.append(f\"- **Latest Snapshot:** {latest}\")\n",
        "        \n",
        "        if pd.api.types.is_datetime64_any_dtype(combined_df['SnapshotDate']):\n",
        "            date_range_days = (pd.to_datetime(latest) - pd.to_datetime(earliest)).days\n",
        "            report_lines.append(f\"- **Time Span:** {date_range_days} days\")\n",
        "        report_lines.append(\"\")\n",
        "    \n",
        "    report_lines.append(\"---\")\n",
        "    report_lines.append(\"\")\n",
        "    \n",
        "    # Risk Distribution Analysis\n",
        "    report_lines.append(\"## Risk Distribution Analysis\")\n",
        "    report_lines.append(\"\")\n",
        "    \n",
        "    if 'RiskBand' in shaped_df.columns:\n",
        "        risk_dist = shaped_df['RiskBand'].value_counts().sort_index()\n",
        "        total = len(shaped_df)\n",
        "        \n",
        "        report_lines.append(\"### Risk Band Distribution\")\n",
        "        report_lines.append(\"\")\n",
        "        report_lines.append(\"| Risk Band | Count | Percentage |\")\n",
        "        report_lines.append(\"|-----------|-------|------------|\")\n",
        "        for band, count in risk_dist.items():\n",
        "            pct = (count / total) * 100\n",
        "            report_lines.append(f\"| {band} | {count:,} | {pct:.2f}% |\")\n",
        "        report_lines.append(\"\")\n",
        "        \n",
        "        # Interpretation\n",
        "        report_lines.append(\"**Risk Band Definitions:**\")\n",
        "        report_lines.append(\"\")\n",
        "        report_lines.append(\"- **A (Low Risk):** Churn probability < 33%\")\n",
        "        report_lines.append(\"- **B (Medium Risk):** Churn probability 33-66%\")\n",
        "        report_lines.append(\"- **C (High Risk):** Churn probability > 66%\")\n",
        "        report_lines.append(\"\")\n",
        "    \n",
        "    if 'ChurnRiskPct' in shaped_df.columns:\n",
        "        report_lines.append(\"### Churn Risk Statistics\")\n",
        "        report_lines.append(\"\")\n",
        "        risk_stats = shaped_df['ChurnRiskPct'].describe()\n",
        "        report_lines.append(\"| Metric | Value |\")\n",
        "        report_lines.append(\"|--------|-------|\")\n",
        "        report_lines.append(f\"| Mean | {risk_stats['mean']:.4f} |\")\n",
        "        report_lines.append(f\"| Median | {risk_stats['50%']:.4f} |\")\n",
        "        report_lines.append(f\"| Standard Deviation | {risk_stats['std']:.4f} |\")\n",
        "        report_lines.append(f\"| Minimum | {risk_stats['min']:.4f} |\")\n",
        "        report_lines.append(f\"| Maximum | {risk_stats['max']:.4f} |\")\n",
        "        report_lines.append(f\"| 25th Percentile | {risk_stats['25%']:.4f} |\")\n",
        "        report_lines.append(f\"| 75th Percentile | {risk_stats['75%']:.4f} |\")\n",
        "        report_lines.append(\"\")\n",
        "        \n",
        "        # High-risk customers\n",
        "        high_risk_count = len(shaped_df[shaped_df['ChurnRiskPct'] >= 0.66])\n",
        "        medium_risk_count = len(shaped_df[(shaped_df['ChurnRiskPct'] >= 0.33) & (shaped_df['ChurnRiskPct'] < 0.66)])\n",
        "        low_risk_count = len(shaped_df[shaped_df['ChurnRiskPct'] < 0.33])\n",
        "        \n",
        "        report_lines.append(\"### Risk Segmentation\")\n",
        "        report_lines.append(\"\")\n",
        "        report_lines.append(f\"- **High Risk (≥66%):** {high_risk_count:,} customers ({(high_risk_count/total_customers)*100:.1f}%)\")\n",
        "        report_lines.append(f\"- **Medium Risk (33-66%):** {medium_risk_count:,} customers ({(medium_risk_count/total_customers)*100:.1f}%)\")\n",
        "        report_lines.append(f\"- **Low Risk (<33%):** {low_risk_count:,} customers ({(low_risk_count/total_customers)*100:.1f}%)\")\n",
        "        report_lines.append(\"\")\n",
        "    \n",
        "    report_lines.append(\"---\")\n",
        "    report_lines.append(\"\")\n",
        "    \n",
        "    # Customer Status Analysis\n",
        "    if 'Status' in shaped_df.columns:\n",
        "        report_lines.append(\"## Customer Status Analysis\")\n",
        "        report_lines.append(\"\")\n",
        "        status_dist = shaped_df['Status'].value_counts()\n",
        "        report_lines.append(\"### Status Distribution\")\n",
        "        report_lines.append(\"\")\n",
        "        report_lines.append(\"| Status | Count | Percentage |\")\n",
        "        report_lines.append(\"|--------|-------|------------|\")\n",
        "        for status, count in status_dist.items():\n",
        "            pct = (count / total_customers) * 100\n",
        "            report_lines.append(f\"| {status} | {count:,} | {pct:.2f}% |\")\n",
        "        report_lines.append(\"\")\n",
        "        \n",
        "        report_lines.append(\"**Status Definitions:**\")\n",
        "        report_lines.append(\"\")\n",
        "        report_lines.append(\"- **New:** Customer's first purchase within last 90 days\")\n",
        "        report_lines.append(\"- **Active:** Last purchase within last 90 days\")\n",
        "        report_lines.append(\"- **Churned:** Last purchase 91-180 days ago\")\n",
        "        report_lines.append(\"- **Reactivated:** Last purchase more than 180 days ago, but has recent activity\")\n",
        "        report_lines.append(\"\")\n",
        "        \n",
        "        # Cross-tabulation: Status vs Risk\n",
        "        if 'RiskBand' in shaped_df.columns:\n",
        "            report_lines.append(\"### Status vs Risk Band Cross-Analysis\")\n",
        "            report_lines.append(\"\")\n",
        "            crosstab = pd.crosstab(shaped_df['Status'], shaped_df['RiskBand'], margins=True)\n",
        "            report_lines.append(\"| Status | A (Low) | B (Medium) | C (High) | Total |\")\n",
        "            report_lines.append(\"|--------|---------|------------|----------|-------|\")\n",
        "            if crosstab.index is not None and len(crosstab.index) > 0:\n",
        "                assert crosstab.index is not None  # Type guard\n",
        "                index_values = crosstab.index  # Store in variable for type checker\n",
        "                for status in index_values[:-1]:  # Exclude 'All' row\n",
        "                    row = crosstab.loc[status]\n",
        "                    all_value = row.get('All', 0)\n",
        "                    report_lines.append(f\"| {status} | {int(row.get('A', 0)):,} | {int(row.get('B', 0)):,} | {int(row.get('C', 0)):,} | {int(all_value):,} |\")\n",
        "            report_lines.append(\"\")\n",
        "        \n",
        "        report_lines.append(\"---\")\n",
        "        report_lines.append(\"\")\n",
        "    \n",
        "    # Key Performance Indicators\n",
        "    report_lines.append(\"## Key Performance Indicators (KPIs)\")\n",
        "    report_lines.append(\"\")\n",
        "    \n",
        "    kpis = []\n",
        "    \n",
        "    if 'ChurnRiskPct' in shaped_df.columns:\n",
        "        avg_risk = shaped_df['ChurnRiskPct'].mean()\n",
        "        kpis.append((\"Average Churn Risk\", f\"{avg_risk:.2%}\"))\n",
        "        \n",
        "        high_risk_pct = (len(shaped_df[shaped_df['ChurnRiskPct'] >= 0.66]) / total_customers) * 100\n",
        "        kpis.append((\"High Risk Customer Rate\", f\"{high_risk_pct:.2f}%\"))\n",
        "    \n",
        "    if 'Status' in shaped_df.columns:\n",
        "        churned_pct = (len(shaped_df[shaped_df['Status'] == 'Churned']) / total_customers) * 100\n",
        "        kpis.append((\"Churned Customer Rate\", f\"{churned_pct:.2f}%\"))\n",
        "        \n",
        "        active_pct = (len(shaped_df[shaped_df['Status'] == 'Active']) / total_customers) * 100\n",
        "        kpis.append((\"Active Customer Rate\", f\"{active_pct:.2f}%\"))\n",
        "    \n",
        "    if 'RiskBand' in shaped_df.columns and 'Status' in shaped_df.columns:\n",
        "        # High-risk active customers (immediate action needed)\n",
        "        high_risk_active = len(shaped_df[(shaped_df['RiskBand'] == 'C') & (shaped_df['Status'] == 'Active')])\n",
        "        kpis.append((\"High-Risk Active Customers (Action Required)\", f\"{high_risk_active:,}\"))\n",
        "    \n",
        "    report_lines.append(\"| KPI | Value |\")\n",
        "    report_lines.append(\"|-----|-------|\")\n",
        "    for kpi_name, kpi_value in kpis:\n",
        "        report_lines.append(f\"| {kpi_name} | {kpi_value} |\")\n",
        "    report_lines.append(\"\")\n",
        "    report_lines.append(\"---\")\n",
        "    report_lines.append(\"\")\n",
        "    \n",
        "    # Sample Data\n",
        "    report_lines.append(\"## Sample Data\")\n",
        "    report_lines.append(\"\")\n",
        "    report_lines.append(\"### High-Risk Customers (Top 10)\")\n",
        "    report_lines.append(\"\")\n",
        "    \n",
        "    if 'ChurnRiskPct' in shaped_df.columns:\n",
        "        high_risk_sample = shaped_df.nlargest(10, 'ChurnRiskPct')[\n",
        "            ['CustomerId', 'AccountName', 'ChurnRiskPct', 'RiskBand', 'Status'] + \n",
        "            (['Reason_1', 'Reason_2', 'Reason_3'] if all(c in shaped_df.columns for c in ['Reason_1', 'Reason_2', 'Reason_3']) else [])\n",
        "        ]\n",
        "        \n",
        "        # Convert to markdown table\n",
        "        report_lines.append(\"| Customer ID | Account Name | Risk % | Risk Band | Status | Top Reasons |\")\n",
        "        report_lines.append(\"|-------------|--------------|--------|-----------|--------|------------|\")\n",
        "        for _, row in high_risk_sample.iterrows():\n",
        "            reasons = \"\"\n",
        "            if 'Reason_1' in row:\n",
        "                reasons = f\"{row.get('Reason_1', 'N/A')[:50]}...\"\n",
        "            report_lines.append(f\"| {row.get('CustomerId', 'N/A')} | {str(row.get('AccountName', 'N/A'))[:30]} | {row.get('ChurnRiskPct', 0):.2%} | {row.get('RiskBand', 'N/A')} | {row.get('Status', 'N/A')} | {reasons} |\")\n",
        "        report_lines.append(\"\")\n",
        "    \n",
        "    report_lines.append(\"---\")\n",
        "    report_lines.append(\"\")\n",
        "    \n",
        "    # Business Insights\n",
        "    report_lines.append(\"## Business Insights & Recommendations\")\n",
        "    report_lines.append(\"\")\n",
        "    \n",
        "    insights = []\n",
        "    \n",
        "    if 'RiskBand' in shaped_df.columns:\n",
        "        high_risk_count = len(shaped_df[shaped_df['RiskBand'] == 'C'])\n",
        "        if high_risk_count > 0:\n",
        "            insights.append(f\"**{high_risk_count:,} customers ({high_risk_count/total_customers*100:.1f}%) are classified as high-risk.** \"\n",
        "                          f\"These customers require immediate attention and proactive retention efforts.\")\n",
        "    \n",
        "    if 'Status' in shaped_df.columns and 'RiskBand' in shaped_df.columns:\n",
        "        high_risk_active = len(shaped_df[(shaped_df['RiskBand'] == 'C') & (shaped_df['Status'] == 'Active')])\n",
        "        if high_risk_active > 0:\n",
        "            insights.append(f\"**{high_risk_active:,} active customers are at high risk of churning.** \"\n",
        "                          f\"This represents an opportunity for proactive intervention before they become inactive.\")\n",
        "    \n",
        "    if 'ChurnRiskPct' in shaped_df.columns:\n",
        "        avg_risk = shaped_df['ChurnRiskPct'].mean()\n",
        "        if avg_risk > 0.5:\n",
        "            insights.append(f\"**The average churn risk across all customers is {avg_risk:.1%}.** \"\n",
        "                          f\"This indicates a significant portion of the customer base may be at risk.\")\n",
        "    \n",
        "    if 'Reason_1' in shaped_df.columns:\n",
        "        # Most common reasons\n",
        "        high_risk_df = shaped_df[shaped_df['RiskBand'] == 'C']\n",
        "        if len(high_risk_df) > 0 and 'Reason_1' in high_risk_df.columns:\n",
        "            reason_series: pd.Series = pd.Series(high_risk_df['Reason_1'])  # Ensure it's a Series\n",
        "            top_reasons = reason_series.value_counts().head(3)\n",
        "            if len(top_reasons) > 0:\n",
        "                insights.append(\"**Top risk factors for high-risk customers:**\")\n",
        "            for i, (reason, count) in enumerate(top_reasons.items(), 1):\n",
        "                insights.append(f\"  {i}. {reason} ({count} customers)\")\n",
        "    \n",
        "    for insight in insights:\n",
        "        report_lines.append(insight)\n",
        "        report_lines.append(\"\")\n",
        "    \n",
        "    report_lines.append(\"### Recommended Actions\")\n",
        "    report_lines.append(\"\")\n",
        "    report_lines.append(\"1. **Immediate Action:** Contact high-risk active customers (Risk Band C, Status: Active) with retention offers\")\n",
        "    report_lines.append(\"2. **Monitoring:** Track medium-risk customers (Risk Band B) for early warning signs\")\n",
        "    report_lines.append(\"3. **Analysis:** Review top risk factors to identify common patterns and root causes\")\n",
        "    report_lines.append(\"4. **Engagement:** Develop targeted retention campaigns based on customer status and risk level\")\n",
        "    report_lines.append(\"\")\n",
        "    report_lines.append(\"---\")\n",
        "    report_lines.append(\"\")\n",
        "    \n",
        "    # Technical Details\n",
        "    report_lines.append(\"## Technical Details\")\n",
        "    report_lines.append(\"\")\n",
        "    report_lines.append(\"### Model Outputs\")\n",
        "    report_lines.append(\"\")\n",
        "    report_lines.append(\"- **ChurnRiskPct:** Probability of churn within 90 days (0.0 to 1.0)\")\n",
        "    report_lines.append(\"- **RiskBand:** Categorical risk classification (A=Low, B=Medium, C=High)\")\n",
        "    report_lines.append(\"- **Reason_1, Reason_2, Reason_3:** Top contributing factors to the churn risk prediction\")\n",
        "    report_lines.append(\"\")\n",
        "    report_lines.append(\"### Data Processing\")\n",
        "    report_lines.append(\"\")\n",
        "    report_lines.append(\"- **Scoring Method:** Batch processing of historical snapshots\")\n",
        "    report_lines.append(\"- **Output Format:** CSV files compatible with Power BI and SQL\")\n",
        "    report_lines.append(\"- **SQL View:** Data shaped to match `dbo.vwCustomerCurrent` view structure\")\n",
        "    report_lines.append(\"\")\n",
        "    \n",
        "    # Write report\n",
        "    report_text = \"\\n\".join(report_lines)\n",
        "    output_path.write_text(report_text, encoding='utf-8')\n",
        "    \n",
        "    print(f\"✓ Model report generated: {output_path}\")\n",
        "    print(f\"  Report length: {len(report_lines)} lines\")\n",
        "    \n",
        "    return report_text\n",
        "\n",
        "# Generate the report (run this after shaped_df is created in section 6)\n",
        "# MODEL_REPORT_OUTPUT = OUTPUT_DIR / \"model_report.md\"\n",
        "# print(\"Generating comprehensive model report...\")\n",
        "# print(\"This may take a moment...\")\n",
        "# report_text = generate_model_report(combined_df, shaped_df, model, MODEL_REPORT_OUTPUT)\n",
        "# print(f\"\\n✓ Model report saved to: {MODEL_REPORT_OUTPUT}\")\n",
        "# print(f\"  Open this file to view the business-ready analysis report\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "load-model",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model (this will be cached for subsequent scoring)\n",
        "print(\"Loading model...\")\n",
        "model = load_model()\n",
        "print(\"✓ Model loaded successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "score-files",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Score each file and combine results\n",
        "all_scored = []\n",
        "start_time = time.time()\n",
        "\n",
        "for idx, df in enumerate(all_dataframes, 1):\n",
        "    file_start = time.time()\n",
        "    source_file = df['SourceFile'].iloc[0]\n",
        "    print(f\"\\n[{idx}/{len(all_dataframes)}] Scoring {source_file}...\")\n",
        "    \n",
        "    # Score this dataframe\n",
        "    scored_df = score_customers(df)\n",
        "    \n",
        "    # Add source file metadata (if not already present)\n",
        "    if 'SourceFile' not in scored_df.columns:\n",
        "        scored_df['SourceFile'] = source_file\n",
        "    \n",
        "    # Add processing timestamp\n",
        "    scored_df['ScoredAt'] = datetime.now()\n",
        "    \n",
        "    all_scored.append(scored_df)\n",
        "    \n",
        "    file_time = time.time() - file_start\n",
        "    print(f\"  ✓ Scored {len(scored_df):,} records in {file_time:.2f} seconds\")\n",
        "\n",
        "# Combine all results\n",
        "if len(all_scored) == 1:\n",
        "    combined_df = all_scored[0]\n",
        "else:\n",
        "    print(f\"\\nCombining {len(all_scored)} files...\")\n",
        "    combined_df = pd.concat(all_scored, ignore_index=True)\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"✓ Scoring completed in {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")\n",
        "print(f\"Total records scored: {len(combined_df):,}\")\n",
        "print(f\"{'='*60}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "explore",
      "metadata": {},
      "source": [
        "## 5) Explore Scored Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "summary-stats",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary statistics\n",
        "print(\"SCORING SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Total records: {len(combined_df):,}\")\n",
        "\n",
        "if 'RiskBand' in combined_df.columns:\n",
        "    print(\"\\nRisk Band Distribution:\")\n",
        "    risk_dist = combined_df['RiskBand'].value_counts()\n",
        "    for band, count in risk_dist.items():\n",
        "        pct = (count / len(combined_df)) * 100\n",
        "        print(f\"  {band}: {count:,} ({pct:.1f}%)\")\n",
        "\n",
        "if 'ChurnRiskPct' in combined_df.columns:\n",
        "    print(\"\\nChurn Risk Statistics:\")\n",
        "    print(f\"  Mean: {combined_df['ChurnRiskPct'].mean():.4f}\")\n",
        "    print(f\"  Median: {combined_df['ChurnRiskPct'].median():.4f}\")\n",
        "    print(f\"  Min: {combined_df['ChurnRiskPct'].min():.4f}\")\n",
        "    print(f\"  Max: {combined_df['ChurnRiskPct'].max():.4f}\")\n",
        "\n",
        "if 'SnapshotDate' in combined_df.columns:\n",
        "    print(\"\\nDate Range:\")\n",
        "    print(f\"  Earliest: {combined_df['SnapshotDate'].min()}\")\n",
        "    print(f\"  Latest: {combined_df['SnapshotDate'].max()}\")\n",
        "\n",
        "if 'SourceFile' in combined_df.columns and combined_df['SourceFile'].nunique() > 1:\n",
        "    print(\"\\nFiles Processed:\")\n",
        "    file_counts = combined_df['SourceFile'].value_counts()\n",
        "    for filename, count in file_counts.items():\n",
        "        print(f\"  {filename}: {count:,} records\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "preview-data",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preview the scored data\n",
        "print(\"Preview of scored data:\")\n",
        "print(f\"Columns: {len(combined_df.columns)}\")\n",
        "print(f\"\\nColumn names:\")\n",
        "for col in combined_df.columns:\n",
        "    print(f\"  - {col}\")\n",
        "\n",
        "print(\"\\nFirst few rows:\")\n",
        "display(combined_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "shape",
      "metadata": {},
      "source": [
        "## 6) Shape for SQL View"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "shape-function",
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_status(row):\n",
        "    \"\"\"Calculate customer status like SQL fnCalculateStatus.\"\"\"\n",
        "    first_purchase = pd.to_datetime(row.get('FirstPurchaseDate'), errors='coerce')\n",
        "    last_purchase = pd.to_datetime(row.get('LastPurchaseDate'), errors='coerce')\n",
        "    snapshot = pd.to_datetime(row.get('SnapshotDate'), errors='coerce')\n",
        "    \n",
        "    if pd.isna(first_purchase) or pd.isna(last_purchase) or pd.isna(snapshot):\n",
        "        return 'Unknown'\n",
        "    \n",
        "    # Days since first purchase\n",
        "    days_since_first = (snapshot - first_purchase).days\n",
        "    \n",
        "    # Days since last purchase\n",
        "    days_since_last = (snapshot - last_purchase).days\n",
        "    \n",
        "    if days_since_first <= 90:\n",
        "        return 'New'\n",
        "    elif days_since_last <= 90:\n",
        "        return 'Active'\n",
        "    elif days_since_last <= 180:\n",
        "        return 'Churned'\n",
        "    else:\n",
        "        return 'Reactivated'\n",
        "\n",
        "def shape_like_sql_view(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Shape data to match SQL view dbo.vwCustomerCurrent.\"\"\"\n",
        "    print(\"Shaping data to match SQL view structure...\")\n",
        "    \n",
        "    # 1. Get latest snapshot per customer (like ROW_NUMBER window function)\n",
        "    if 'SnapshotDate' in df.columns:\n",
        "        df = df.sort_values(['CustomerId', 'SnapshotDate'], ascending=[True, False])\n",
        "        df = df.drop_duplicates(subset=['CustomerId'], keep='first')\n",
        "        print(f\"  ✓ Latest snapshot per customer: {len(df):,} records\")\n",
        "    \n",
        "    # 2. Calculate Status\n",
        "    if 'FirstPurchaseDate' in df.columns and 'LastPurchaseDate' in df.columns:\n",
        "        df['Status'] = df.apply(calculate_status, axis=1)\n",
        "        print(f\"  ✓ Calculated Status column\")\n",
        "    else:\n",
        "        print(\"  ⚠ Warning: Missing FirstPurchaseDate or LastPurchaseDate - Status will be Unknown\")\n",
        "        df['Status'] = 'Unknown'\n",
        "    \n",
        "    # 3. Remove SourceFile (local processing artifact)\n",
        "    if 'SourceFile' in df.columns:\n",
        "        df = df.drop(columns=['SourceFile'])\n",
        "        print(f\"  ✓ Removed SourceFile column\")\n",
        "    \n",
        "    # 4. Order columns to match SQL view (key columns first, then features, then scores)\n",
        "    id_cols = ['CustomerId', 'AccountName', 'Segment', 'CostCenter', 'SnapshotDate']\n",
        "    date_cols = ['FirstPurchaseDate', 'LastPurchaseDate']\n",
        "    status_cols = ['Status']\n",
        "    score_cols = ['ChurnRiskPct', 'RiskBand', 'Reason_1', 'Reason_2', 'Reason_3']\n",
        "    \n",
        "    # Build ordered column list\n",
        "    ordered_cols = []\n",
        "    for col_list in [id_cols, date_cols, status_cols, score_cols]:\n",
        "        for col in col_list:\n",
        "            if col in df.columns:\n",
        "                ordered_cols.append(col)\n",
        "    \n",
        "    # Add remaining columns (features)\n",
        "    remaining_cols = [c for c in df.columns if c not in ordered_cols and c != 'ScoredAt']\n",
        "    ordered_cols.extend(sorted(remaining_cols))\n",
        "    \n",
        "    # Add ScoredAt at the end if present\n",
        "    if 'ScoredAt' in df.columns:\n",
        "        ordered_cols.append('ScoredAt')\n",
        "    \n",
        "    # Reorder columns\n",
        "    df = df[ordered_cols]\n",
        "    print(f\"  ✓ Reordered columns to match SQL view structure\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "print(\"✓ Shape functions defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "apply-shape",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply shaping\n",
        "shaped_df = shape_like_sql_view(combined_df.copy())\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Shaped data: {len(shaped_df):,} records\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "# Show status distribution\n",
        "if 'Status' in shaped_df.columns:\n",
        "    print(\"\\nStatus Distribution:\")\n",
        "    status_dist = shaped_df['Status'].value_counts()\n",
        "    for status, count in status_dist.items():\n",
        "        pct = (count / len(shaped_df)) * 100\n",
        "        print(f\"  {status}: {count:,} ({pct:.1f}%)\")\n",
        "\n",
        "print(\"\\nPreview of shaped data:\")\n",
        "display(shaped_df.head())\n",
        "\n",
        "# Generate the comprehensive model report\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Generating comprehensive model report...\")\n",
        "MODEL_REPORT_OUTPUT = OUTPUT_DIR / \"model_report.md\"\n",
        "report_text = generate_model_report(combined_df, shaped_df, model, MODEL_REPORT_OUTPUT)\n",
        "print(f\"\\n✓ Model report saved to: {MODEL_REPORT_OUTPUT}\")\n",
        "print(f\"  Open this file to view the business-ready analysis report\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "export",
      "metadata": {},
      "source": [
        "## 7) Export Outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "export-combined",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export combined scored data\n",
        "print(f\"Writing combined scored data to {COMBINED_OUTPUT}...\")\n",
        "combined_df.to_csv(COMBINED_OUTPUT, index=False)\n",
        "file_size_mb = COMBINED_OUTPUT.stat().st_size / (1024 * 1024)\n",
        "print(f\"✓ Wrote {len(combined_df):,} records ({file_size_mb:.2f} MB)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "export-shaped",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export shaped data (SQL view format)\n",
        "print(f\"Writing shaped data to {SQL_VIEW_OUTPUT}...\")\n",
        "shaped_df.to_csv(SQL_VIEW_OUTPUT, index=False)\n",
        "file_size_mb = SQL_VIEW_OUTPUT.stat().st_size / (1024 * 1024)\n",
        "print(f\"✓ Wrote {len(shaped_df):,} records ({file_size_mb:.2f} MB)\")\n",
        "print(f\"\\nThis file matches the structure of SQL view: dbo.vwCustomerCurrent\")\n",
        "print(f\"Ready for Power BI exploration!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "copy-docs",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Copy documentation files\n",
        "readme_src = PROJECT_ROOT / \"README.md\"\n",
        "if readme_src.exists():\n",
        "    shutil.copy2(readme_src, OUTPUT_DIR / \"README.md\")\n",
        "    print(\"✓ Copied README.md\")\n",
        "\n",
        "conda_src = PROJECT_ROOT / \"model\" / \"conda.yml\"\n",
        "if conda_src.exists():\n",
        "    shutil.copy2(conda_src, OUTPUT_DIR / \"model_conda.yml\")\n",
        "    print(\"✓ Copied model/conda.yml\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"✓ All outputs exported successfully!\")\n",
        "print(f\"Output directory: {OUTPUT_DIR}\")\n",
        "print(f\"{'='*60}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv (3.13.9)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
