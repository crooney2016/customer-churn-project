{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d96810ae",
      "metadata": {},
      "source": [
        "# Customer Churn Model (XGBoost) with CUBS Features\n",
        "\n",
        "This notebook:\n",
        "- Loads train.csv (Jan-Aug) and validate.csv (Sep-Oct)\n",
        "- Computes churn label from future snapshots\n",
        "- Trains XGBoost classifier with CUBS category features\n",
        "- Outputs scored results with business-readable reasons\n",
        "\n",
        "**Inputs:** train.csv, validate.csv (from DAX query)\n",
        "\n",
        "**Outputs:**\n",
        "- model_quality_report.txt\n",
        "- feature_importance.csv\n",
        "- churn_scores_long.csv\n",
        "- churn_scores_wide_12m.csv\n",
        "- portfolio_summary.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2df736d",
      "metadata": {},
      "source": [
        "## 0) Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f8cba98",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys, platform\n",
        "print(\"python:\", sys.version)\n",
        "print(\"platform:\", platform.platform())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3af68b89",
      "metadata": {},
      "source": [
        "## 1) Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1d2098a",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, confusion_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4929863e",
      "metadata": {},
      "source": [
        "## 2) Load data + normalize column names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "307649f5",
      "metadata": {},
      "outputs": [],
      "source": [
        "TRAIN_PATH = \"train.csv\"\n",
        "VALIDATE_PATH = \"validate.csv\"\n",
        "\n",
        "train_df = pd.read_csv(TRAIN_PATH)\n",
        "validate_df = pd.read_csv(VALIDATE_PATH)\n",
        "\n",
        "def normalize_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Strip whitespace and remove square brackets from column names.\"\"\"\n",
        "    df = df.copy()\n",
        "    df.columns = (\n",
        "        df.columns.astype(str)\n",
        "        .str.strip()\n",
        "        .str.replace(r\"^\\[\", \"\", regex=True)\n",
        "        .str.replace(r\"\\]$\", \"\", regex=True)\n",
        "    )\n",
        "    return df\n",
        "\n",
        "train_df = normalize_cols(train_df)\n",
        "validate_df = normalize_cols(validate_df)\n",
        "\n",
        "# Combine for label computation\n",
        "all_df = pd.concat([train_df, validate_df], ignore_index=True)\n",
        "\n",
        "print(\"Train shape:\", train_df.shape)\n",
        "print(\"Validate shape:\", validate_df.shape)\n",
        "print(\"Combined shape:\", all_df.shape)\n",
        "print(\"Columns:\", list(train_df.columns))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "label-compute",
      "metadata": {},
      "source": [
        "## 3) Compute churn label\n",
        "\n",
        "For each customer-snapshot, check if they have orders in the next 3 monthly snapshots.\n",
        "- WillChurn90 = 1 if no orders in next 90 days (churned)\n",
        "- WillChurn90 = 0 if they had orders (retained)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "label-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parse snapshot dates\n",
        "all_df[\"SnapshotDate\"] = pd.to_datetime(all_df[\"SnapshotDate\"], errors=\"coerce\")\n",
        "\n",
        "# Sort by customer and date\n",
        "all_df = all_df.sort_values([\"CustomerId\", \"SnapshotDate\"]).reset_index(drop=True)\n",
        "\n",
        "def compute_churn_label(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    For each row, look at this customer's next 3 monthly snapshots.\n",
        "    If Orders_CY > 0 in any of them, customer didn't churn (0).\n",
        "    If no future snapshots or all have 0 orders, customer churned (1).\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    df[\"WillChurn90\"] = np.nan\n",
        "    \n",
        "    for cust_id in df[\"CustomerId\"].unique():\n",
        "        cust_mask = df[\"CustomerId\"] == cust_id\n",
        "        cust_rows = df.loc[cust_mask].sort_values(\"SnapshotDate\")\n",
        "        indices = cust_rows.index.tolist()\n",
        "        \n",
        "        for i, idx in enumerate(indices):\n",
        "            # Look at next 3 snapshots for this customer\n",
        "            future_indices = indices[i+1:i+4]\n",
        "            \n",
        "            if len(future_indices) == 0:\n",
        "                # No future data - can't compute label\n",
        "                df.loc[idx, \"WillChurn90\"] = np.nan\n",
        "            else:\n",
        "                # Check if any future snapshot has orders\n",
        "                future_orders = df.loc[future_indices, \"Orders_CY\"].fillna(0).sum()\n",
        "                df.loc[idx, \"WillChurn90\"] = 0 if future_orders > 0 else 1\n",
        "    \n",
        "    return df\n",
        "\n",
        "all_df = compute_churn_label(all_df)\n",
        "\n",
        "# Check label distribution\n",
        "print(\"Label distribution:\")\n",
        "print(all_df[\"WillChurn90\"].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "split-section",
      "metadata": {},
      "source": [
        "## 4) Split back into train/validate\n",
        "\n",
        "Train: Jan-Aug (have future data for labels)\n",
        "Validate: Sep-Oct (have future data for labels)\n",
        "Drop rows without labels (Nov-Dec or missing future)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "split-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop rows without labels\n",
        "labeled_df = all_df.dropna(subset=[\"WillChurn90\"]).copy()\n",
        "labeled_df[\"WillChurn90\"] = labeled_df[\"WillChurn90\"].astype(int)\n",
        "\n",
        "# Split by date: Train = Jan-Aug, Validate = Sep-Oct\n",
        "train_cutoff = pd.Timestamp(\"2025-08-31\")\n",
        "validate_cutoff = pd.Timestamp(\"2025-10-31\")\n",
        "\n",
        "train_df = labeled_df[labeled_df[\"SnapshotDate\"] <= train_cutoff].copy()\n",
        "validate_df = labeled_df[\n",
        "    (labeled_df[\"SnapshotDate\"] > train_cutoff) & \n",
        "    (labeled_df[\"SnapshotDate\"] <= validate_cutoff)\n",
        "].copy()\n",
        "\n",
        "print(f\"Train: {len(train_df)} rows ({train_df['SnapshotDate'].min()} to {train_df['SnapshotDate'].max()})\")\n",
        "print(f\"Validate: {len(validate_df)} rows ({validate_df['SnapshotDate'].min()} to {validate_df['SnapshotDate'].max()})\")\n",
        "print(f\"\\nTrain churn rate: {train_df['WillChurn90'].mean():.2%}\")\n",
        "print(f\"Validate churn rate: {validate_df['WillChurn90'].mean():.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d207484f",
      "metadata": {},
      "source": [
        "## 5) Preprocessing\n",
        "\n",
        "- One-hot encode Segment and CostCenter\n",
        "- Drop ID columns, date columns, and target from features\n",
        "- All CUBS category features pass through as-is"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf589f5c",
      "metadata": {},
      "outputs": [],
      "source": [
        "TARGET_COL = \"WillChurn90\"\n",
        "\n",
        "# All date columns to exclude from features\n",
        "DATE_COLS = [\n",
        "    \"SnapshotDate\", \"FirstPurchaseDate\", \"LastPurchaseDate\",\n",
        "    \"Uniforms_FirstPurchaseDate\", \"Sparring_FirstPurchaseDate\",\n",
        "    \"Belts_FirstPurchaseDate\", \"Bags_FirstPurchaseDate\", \"Customs_FirstPurchaseDate\"\n",
        "]\n",
        "\n",
        "ID_COLS = [\"CustomerId\", \"AccountName\", \"Segment\", \"CostCenter\"]\n",
        "\n",
        "DROP_FROM_FEATURES = [TARGET_COL, \"DataSplit\"] + DATE_COLS\n",
        "\n",
        "def preprocess(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Return model-ready feature matrix.\"\"\"\n",
        "    df = df.copy()\n",
        "    \n",
        "    # Clean categorical columns\n",
        "    if \"AccountName\" in df.columns:\n",
        "        df[\"AccountName\"] = df[\"AccountName\"].fillna(\"\").astype(str).str.strip()\n",
        "    if \"Segment\" in df.columns:\n",
        "        df[\"Segment\"] = df[\"Segment\"].fillna(\"UNKNOWN\").astype(str).str.strip()\n",
        "    if \"CostCenter\" in df.columns:\n",
        "        df[\"CostCenter\"] = df[\"CostCenter\"].fillna(\"UNKNOWN\").astype(str).str.strip()\n",
        "    \n",
        "    # Drop non-feature columns\n",
        "    X = df.drop(columns=[c for c in (DROP_FROM_FEATURES + ID_COLS) if c in df.columns], errors=\"ignore\")\n",
        "    \n",
        "    # One-hot encode Segment and CostCenter\n",
        "    seg = df[\"Segment\"] if \"Segment\" in df.columns else pd.Series([\"UNKNOWN\"] * len(df), index=df.index)\n",
        "    cc = df[\"CostCenter\"] if \"CostCenter\" in df.columns else pd.Series([\"UNKNOWN\"] * len(df), index=df.index)\n",
        "    \n",
        "    X = pd.concat([X, pd.get_dummies(seg, prefix=\"Segment\", drop_first=False)], axis=1)\n",
        "    X = pd.concat([X, pd.get_dummies(cc, prefix=\"CostCenter\", drop_first=False)], axis=1)\n",
        "    \n",
        "    return X"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f52bddc1",
      "metadata": {},
      "source": [
        "## 6) Build train/validate matrices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a68957a8",
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train = preprocess(train_df)\n",
        "y_train = train_df[TARGET_COL].astype(int)\n",
        "\n",
        "X_val = preprocess(validate_df)\n",
        "y_val = validate_df[TARGET_COL].astype(int)\n",
        "\n",
        "# Align columns (handle segments that appear in one but not other)\n",
        "X_val = X_val.reindex(columns=X_train.columns, fill_value=0)\n",
        "\n",
        "print(\"Feature count:\", X_train.shape[1])\n",
        "print(\"Train rows:\", X_train.shape[0])\n",
        "print(\"Validate rows:\", X_val.shape[0])\n",
        "print(\"\\nFeature columns:\")\n",
        "print(list(X_train.columns))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f98165e",
      "metadata": {},
      "source": [
        "## 7) Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1974d60d",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = XGBClassifier(\n",
        "    n_estimators=400,\n",
        "    max_depth=5,\n",
        "    learning_rate=0.05,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    reg_lambda=1.0,\n",
        "    eval_metric=\"logloss\",\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "val_probs = model.predict_proba(X_val)[:, 1]\n",
        "val_preds = (val_probs >= 0.5).astype(int)\n",
        "\n",
        "# Save model and training columns for deployment\n",
        "import os\n",
        "import joblib\n",
        "\n",
        "os.makedirs(\"model\", exist_ok=True)\n",
        "joblib.dump(model, \"model/churn_model.pkl\")\n",
        "joblib.dump(list(X_train.columns), \"model/model_columns.pkl\")\n",
        "\n",
        "print(\"Training complete.\")\n",
        "print(\"Saved to ./model/ folder: churn_model.pkl, model_columns.pkl\")\n",
        "print(\"Upload this folder to Azure ML Studio to register the model.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00bf9a96",
      "metadata": {},
      "source": [
        "## 8) Model quality report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "673dbf80",
      "metadata": {},
      "outputs": [],
      "source": [
        "metrics = {\n",
        "    \"roc_auc\": float(roc_auc_score(y_val, val_probs)),\n",
        "    \"precision\": float(precision_score(y_val, val_preds)),\n",
        "    \"recall\": float(recall_score(y_val, val_preds)),\n",
        "    \"f1\": float(f1_score(y_val, val_preds)),\n",
        "}\n",
        "cm = confusion_matrix(y_val, val_preds)\n",
        "\n",
        "report_lines = [\n",
        "    \"MODEL QUALITY REPORT\",\n",
        "    \"-\" * 60,\n",
        "    f\"ROC AUC   : {metrics['roc_auc']:.6f}\",\n",
        "    f\"Precision : {metrics['precision']:.6f}\",\n",
        "    f\"Recall    : {metrics['recall']:.6f}\",\n",
        "    f\"F1        : {metrics['f1']:.6f}\",\n",
        "    \"\",\n",
        "    \"Confusion Matrix (rows=true, cols=pred):\",\n",
        "    str(cm)\n",
        "]\n",
        "\n",
        "report_text = \"\\n\".join(report_lines)\n",
        "print(report_text)\n",
        "\n",
        "with open(\"model_quality_report.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(report_text)\n",
        "\n",
        "print(\"\\nWrote: model_quality_report.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fbeb79d",
      "metadata": {},
      "source": [
        "## 9) Feature importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c134609",
      "metadata": {},
      "outputs": [],
      "source": [
        "feature_importance = pd.DataFrame({\n",
        "    \"feature\": X_train.columns,\n",
        "    \"importance\": model.feature_importances_\n",
        "}).sort_values(\"importance\", ascending=False)\n",
        "\n",
        "feature_importance.to_csv(\"feature_importance.csv\", index=False)\n",
        "print(\"Wrote: feature_importance.csv\")\n",
        "feature_importance.head(25)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "800fce1e",
      "metadata": {},
      "source": [
        "## 10) Per-customer reasons\n",
        "\n",
        "Uses XGBoost native feature contributions to generate human-readable explanations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d83c1f30",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get feature contributions\n",
        "dm_val = xgb.DMatrix(X_val)\n",
        "contrib = model.get_booster().predict(dm_val, pred_contribs=True)\n",
        "contrib_df = pd.DataFrame(contrib, columns=list(X_train.columns) + [\"BIAS\"])\n",
        "\n",
        "def feature_phrase(name: str) -> str:\n",
        "    \"\"\"Base phrase for a feature (no polarity).\"\"\"\n",
        "    if name.startswith(\"Segment_\"):\n",
        "        return f\"Customer segment is {name.replace('Segment_', '')}\"\n",
        "    if name.startswith(\"CostCenter_\"):\n",
        "        return f\"Cost center is {name.replace('CostCenter_', '')}\"\n",
        "    \n",
        "    mapping = {\n",
        "        # Aggregate features\n",
        "        \"Orders_CY\": \"order count (current year)\",\n",
        "        \"Orders_PY\": \"order count (prior year)\",\n",
        "        \"Orders_Lifetime\": \"lifetime order count\",\n",
        "        \"Spend_CY\": \"spend (current year)\",\n",
        "        \"Spend_PY\": \"spend (prior year)\",\n",
        "        \"Spend_Lifetime\": \"lifetime spend\",\n",
        "        \"Units_CY\": \"units purchased (current year)\",\n",
        "        \"Units_PY\": \"units purchased (prior year)\",\n",
        "        \"Units_Lifetime\": \"lifetime units\",\n",
        "        \"AOV_CY\": \"average order value\",\n",
        "        \"DaysSinceLast\": \"days since last order\",\n",
        "        \"TenureDays\": \"customer tenure (days)\",\n",
        "        \n",
        "        # CUBS category features\n",
        "        \"Uniforms_Units_CY\": \"uniforms units (current year)\",\n",
        "        \"Uniforms_Spend_CY\": \"uniforms spend (current year)\",\n",
        "        \"Uniforms_Orders_CY\": \"uniforms orders (current year)\",\n",
        "        \"Uniforms_Pct_of_Total_CY\": \"uniforms % of total spend\",\n",
        "        \"Uniforms_DaysSinceLast\": \"days since last uniforms order\",\n",
        "        \n",
        "        \"Sparring_Units_CY\": \"sparring units (current year)\",\n",
        "        \"Sparring_Spend_CY\": \"sparring spend (current year)\",\n",
        "        \"Sparring_Orders_CY\": \"sparring orders (current year)\",\n",
        "        \"Sparring_Pct_of_Total_CY\": \"sparring % of total spend\",\n",
        "        \"Sparring_DaysSinceLast\": \"days since last sparring order\",\n",
        "        \n",
        "        \"Belts_Units_CY\": \"belts units (current year)\",\n",
        "        \"Belts_Spend_CY\": \"belts spend (current year)\",\n",
        "        \"Belts_Orders_CY\": \"belts orders (current year)\",\n",
        "        \"Belts_Pct_of_Total_CY\": \"belts % of total spend\",\n",
        "        \"Belts_DaysSinceLast\": \"days since last belts order\",\n",
        "        \n",
        "        \"Bags_Units_CY\": \"bags units (current year)\",\n",
        "        \"Bags_Spend_CY\": \"bags spend (current year)\",\n",
        "        \"Bags_Orders_CY\": \"bags orders (current year)\",\n",
        "        \"Bags_Pct_of_Total_CY\": \"bags % of total spend\",\n",
        "        \"Bags_DaysSinceLast\": \"days since last bags order\",\n",
        "        \n",
        "        \"Customs_Units_CY\": \"customs units (current year)\",\n",
        "        \"Customs_Spend_CY\": \"customs spend (current year)\",\n",
        "        \"Customs_Orders_CY\": \"customs orders (current year)\",\n",
        "        \"Customs_Pct_of_Total_CY\": \"customs % of total spend\",\n",
        "        \"Customs_DaysSinceLast\": \"days since last customs order\",\n",
        "        \n",
        "        # Breadth features\n",
        "        \"CUBS_Categories_Active_CY\": \"product categories active (current year)\",\n",
        "        \"CUBS_Categories_Active_PY\": \"product categories active (prior year)\",\n",
        "        \"CUBS_Categories_Ever\": \"product categories ever purchased\",\n",
        "    }\n",
        "    return mapping.get(name, name.replace(\"_\", \" \"))\n",
        "\n",
        "\n",
        "def reason_text(feature: str, mode: str) -> str:\n",
        "    \"\"\"Generate reason text. mode: 'risk' (drivers) or 'safe' (protective).\"\"\"\n",
        "    base = feature_phrase(feature)\n",
        "    \n",
        "    # Segment/CostCenter: keep as-is\n",
        "    if feature.startswith(\"Segment_\") or feature.startswith(\"CostCenter_\"):\n",
        "        return base\n",
        "    \n",
        "    # Features where HIGH value = LOW risk (protective)\n",
        "    high_is_good = [\n",
        "        \"Orders_CY\", \"Orders_PY\", \"Orders_Lifetime\",\n",
        "        \"Spend_CY\", \"Spend_PY\", \"Spend_Lifetime\",\n",
        "        \"Units_CY\", \"Units_PY\", \"Units_Lifetime\",\n",
        "        \"AOV_CY\", \"TenureDays\",\n",
        "        \"Uniforms_Units_CY\", \"Uniforms_Spend_CY\", \"Uniforms_Orders_CY\",\n",
        "        \"Sparring_Units_CY\", \"Sparring_Spend_CY\", \"Sparring_Orders_CY\",\n",
        "        \"Belts_Units_CY\", \"Belts_Spend_CY\", \"Belts_Orders_CY\",\n",
        "        \"Bags_Units_CY\", \"Bags_Spend_CY\", \"Bags_Orders_CY\",\n",
        "        \"Customs_Units_CY\", \"Customs_Spend_CY\", \"Customs_Orders_CY\",\n",
        "        \"CUBS_Categories_Active_CY\", \"CUBS_Categories_Active_PY\", \"CUBS_Categories_Ever\",\n",
        "    ]\n",
        "    \n",
        "    # Features where HIGH value = HIGH risk\n",
        "    high_is_bad = [\n",
        "        \"DaysSinceLast\",\n",
        "        \"Uniforms_DaysSinceLast\", \"Sparring_DaysSinceLast\",\n",
        "        \"Belts_DaysSinceLast\", \"Bags_DaysSinceLast\", \"Customs_DaysSinceLast\",\n",
        "    ]\n",
        "    \n",
        "    if mode == \"risk\":\n",
        "        if feature in high_is_good:\n",
        "            return f\"Low {base}\"\n",
        "        if feature in high_is_bad:\n",
        "            return f\"High {base}\"\n",
        "        return f\"Unfavorable {base}\"\n",
        "    else:  # safe\n",
        "        if feature in high_is_good:\n",
        "            return f\"High {base}\"\n",
        "        if feature in high_is_bad:\n",
        "            return f\"Low {base}\"\n",
        "        return f\"Favorable {base}\"\n",
        "\n",
        "\n",
        "def risk_band(p: float) -> str:\n",
        "    if p >= 0.7:\n",
        "        return \"A - High Risk\"\n",
        "    elif p >= 0.3:\n",
        "        return \"B - Medium Risk\"\n",
        "    else:\n",
        "        return \"C - Low Risk\"\n",
        "\n",
        "\n",
        "def top_reasons(row_contrib: pd.Series, risk: float, n: int = 3) -> list:\n",
        "    s = row_contrib.drop(labels=[\"BIAS\"], errors=\"ignore\")\n",
        "    \n",
        "    if risk >= 0.7:\n",
        "        # High risk: top positive contributors\n",
        "        feats = s.sort_values(ascending=False).head(n).index.tolist()\n",
        "        return [reason_text(f, \"risk\") for f in feats]\n",
        "    \n",
        "    if risk < 0.3:\n",
        "        # Low risk: top negative contributors (protective)\n",
        "        feats = s.sort_values(ascending=True).head(n).index.tolist()\n",
        "        return [reason_text(f, \"safe\") for f in feats]\n",
        "    \n",
        "    # Medium: 2 risk drivers + 1 protective\n",
        "    pos = s.sort_values(ascending=False).head(2).index.tolist()\n",
        "    neg = s.sort_values(ascending=True).head(1).index.tolist()\n",
        "    return ([reason_text(f, \"risk\") for f in pos] + [reason_text(f, \"safe\") for f in neg])[:3]\n",
        "\n",
        "\n",
        "# Generate reasons for all validation rows\n",
        "reasons_rows = []\n",
        "for i in range(len(X_val)):\n",
        "    reasons_rows.append(top_reasons(contrib_df.iloc[i], float(val_probs[i]), n=3))\n",
        "\n",
        "reasons = pd.DataFrame(reasons_rows, columns=[\"Reason_1\", \"Reason_2\", \"Reason_3\"])\n",
        "print(f\"Generated reasons for {len(reasons)} rows\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5302c543",
      "metadata": {},
      "source": [
        "## 11) Long output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2055d9d8",
      "metadata": {},
      "outputs": [],
      "source": [
        "validate_out = validate_df.copy()\n",
        "validate_out[\"AccountName\"] = validate_out[\"AccountName\"].fillna(\"\").astype(str).str.strip()\n",
        "validate_out[\"Segment\"] = validate_out[\"Segment\"].fillna(\"UNKNOWN\").astype(str).str.strip()\n",
        "validate_out[\"CostCenter\"] = validate_out[\"CostCenter\"].fillna(\"UNKNOWN\").astype(str).str.strip()\n",
        "\n",
        "final_long = validate_out[[\"CustomerId\", \"AccountName\", \"Segment\", \"CostCenter\", \"SnapshotDate\"]].copy()\n",
        "final_long[\"ChurnRiskPct\"] = val_probs\n",
        "final_long[\"RiskBand\"] = final_long[\"ChurnRiskPct\"].apply(lambda p: risk_band(float(p)))\n",
        "\n",
        "final_long = pd.concat([final_long.reset_index(drop=True), reasons.reset_index(drop=True)], axis=1)\n",
        "\n",
        "final_long.to_csv(\"churn_scores_long.csv\", index=False)\n",
        "print(\"Wrote: churn_scores_long.csv\")\n",
        "final_long.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2119777d",
      "metadata": {},
      "source": [
        "## 12) Wide output (trailing 12 months)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49bf71b9",
      "metadata": {},
      "outputs": [],
      "source": [
        "final_long = final_long.copy()\n",
        "final_long[\"SnapshotMonth\"] = pd.to_datetime(final_long[\"SnapshotDate\"]).dt.to_period(\"M\").astype(str)\n",
        "\n",
        "max_month = pd.to_datetime(final_long[\"SnapshotDate\"]).max().to_period(\"M\")\n",
        "months = pd.period_range(end=max_month, periods=12, freq=\"M\").astype(str).tolist()\n",
        "\n",
        "final_12 = final_long[final_long[\"SnapshotMonth\"].isin(months)].copy()\n",
        "\n",
        "wide_scores = (\n",
        "    final_12\n",
        "    .pivot_table(\n",
        "        index=[\"CustomerId\", \"AccountName\", \"Segment\", \"CostCenter\"],\n",
        "        columns=\"SnapshotMonth\",\n",
        "        values=\"ChurnRiskPct\",\n",
        "        aggfunc=\"max\"\n",
        "    )\n",
        "    .reindex(columns=months)\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "start_col = months[0]\n",
        "current_col = months[-1]\n",
        "\n",
        "wide_scores[\"Risk_Start_12m\"] = wide_scores[start_col] if start_col in wide_scores.columns else np.nan\n",
        "wide_scores[\"Risk_Current\"] = wide_scores[current_col] if current_col in wide_scores.columns else np.nan\n",
        "wide_scores[\"Risk_Trend_12m\"] = wide_scores[\"Risk_Current\"] - wide_scores[\"Risk_Start_12m\"]\n",
        "\n",
        "def trend_dir(x):\n",
        "    if pd.isna(x):\n",
        "        return \"Unknown\"\n",
        "    if x >= 0.05:\n",
        "        return \"Up\"\n",
        "    if x <= -0.05:\n",
        "        return \"Down\"\n",
        "    return \"Flat\"\n",
        "\n",
        "wide_scores[\"TrendDirection\"] = wide_scores[\"Risk_Trend_12m\"].apply(trend_dir)\n",
        "wide_scores[\"Risk_Avg_12m\"] = wide_scores[months].mean(axis=1, skipna=True)\n",
        "wide_scores[\"Risk_Median_12m\"] = wide_scores[months].median(axis=1, skipna=True)\n",
        "wide_scores[\"RiskBand_Current\"] = wide_scores[\"Risk_Current\"].apply(lambda p: risk_band(float(p)) if pd.notna(p) else \"Unknown\")\n",
        "\n",
        "# Add latest reasons\n",
        "latest_reasons = (\n",
        "    final_12.sort_values([\"CustomerId\", \"SnapshotDate\"])\n",
        "    .groupby(\"CustomerId\")\n",
        "    .tail(1)[[\"CustomerId\", \"Reason_1\", \"Reason_2\", \"Reason_3\"]]\n",
        ")\n",
        "\n",
        "wide_out = wide_scores.merge(latest_reasons, on=\"CustomerId\", how=\"left\")\n",
        "wide_out.to_csv(\"churn_scores_wide_12m.csv\", index=False)\n",
        "\n",
        "print(\"Wrote: churn_scores_wide_12m.csv\")\n",
        "wide_out.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8db6b550",
      "metadata": {},
      "source": [
        "## 13) Portfolio summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db410450",
      "metadata": {},
      "outputs": [],
      "source": [
        "portfolio = final_long.copy()\n",
        "\n",
        "summary = pd.DataFrame({\n",
        "    \"Metric\": [\n",
        "        \"Rows (customer-snapshots)\",\n",
        "        \"Unique customers\",\n",
        "        \"Avg churn risk (all rows)\",\n",
        "        \"Median churn risk (all rows)\",\n",
        "        \"High risk count (A)\",\n",
        "        \"Medium risk count (B)\",\n",
        "        \"Low risk count (C)\",\n",
        "        \"High risk pct (A)\",\n",
        "        \"Medium risk pct (B)\",\n",
        "        \"Low risk pct (C)\",\n",
        "    ],\n",
        "    \"Value\": [\n",
        "        int(len(portfolio)),\n",
        "        int(portfolio[\"CustomerId\"].nunique()),\n",
        "        float(portfolio[\"ChurnRiskPct\"].mean()),\n",
        "        float(portfolio[\"ChurnRiskPct\"].median()),\n",
        "        int((portfolio[\"RiskBand\"] == \"A - High Risk\").sum()),\n",
        "        int((portfolio[\"RiskBand\"] == \"B - Medium Risk\").sum()),\n",
        "        int((portfolio[\"RiskBand\"] == \"C - Low Risk\").sum()),\n",
        "        float((portfolio[\"RiskBand\"] == \"A - High Risk\").mean()),\n",
        "        float((portfolio[\"RiskBand\"] == \"B - Medium Risk\").mean()),\n",
        "        float((portfolio[\"RiskBand\"] == \"C - Low Risk\").mean()),\n",
        "    ]\n",
        "})\n",
        "\n",
        "summary.to_csv(\"portfolio_summary.csv\", index=False)\n",
        "print(\"Wrote: portfolio_summary.csv\")\n",
        "summary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91d2ed00",
      "metadata": {},
      "source": [
        "## 14) File list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd369eea",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "for fn in sorted(os.listdir(\".\")):\n",
        "    if fn.endswith(\".csv\") or fn.endswith(\".txt\"):\n",
        "        print(fn)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10 - SDK v2",
      "language": "python",
      "name": "python310-sdkv2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
